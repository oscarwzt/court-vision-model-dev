{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from video_utils import *\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"yolov8l-seg.pt\"\n",
    "video_path = \"video_test_dataset/1/3.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(model_path)\n",
    "cap, fps, frame_width, frame_height, total_frames = initialize_video_capture(video_path=video_path, skip_to_sec = 0)\n",
    "out, output_path = initialize_video_writer(fps = fps,\n",
    "                                           video_dimension= (frame_width, frame_height),\n",
    "                                           video_path=video_path,\n",
    "                                           )\n",
    "all_players = []\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Get segmentation results for the current frame\n",
    "    results = model(frame, verbose=False, device=device)\n",
    "    \n",
    "    # Process each result\n",
    "    for r in results:\n",
    "        all_masks = r.masks.data.cpu().numpy() # Assuming this gives you binary masks for each detected object\n",
    "        all_boxes = r.boxes.xyxy.cpu().numpy().astype(int)\n",
    "        classes = r.boxes.cls.cpu().numpy().astype(int)\n",
    "        class_names = [model.names[cls] for cls in classes]\n",
    "        \n",
    "        for mask, box, class_name in zip(all_masks, all_boxes, class_names):\n",
    "            if class_name == \"person\":\n",
    "                mask = cv2.resize(mask, (frame_width, frame_height))\n",
    "                mask = np.stack([mask, mask, mask], axis=2)\n",
    "                mask = mask.astype(np.uint8)\n",
    "                masked_img = mask * frame\n",
    "                \n",
    "                x1, y1, x2, y2 = box\n",
    "                masked_img = masked_img[y1:y2, x1:x2]\n",
    "                masked_img = cv2.cvtColor(masked_img, cv2.COLOR_BGR2RGB)\n",
    "                all_players.append(masked_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homedir/ugrad/z/zw2688/bigdata/environments/vision_cuda111/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/homedir/ugrad/z/zw2688/bigdata/environments/vision_cuda111/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /homedir/ugrad/z/zw2688/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n",
      "100%|██████████| 21.1M/21.1M [00:00<00:00, 257MB/s]\n"
     ]
    }
   ],
   "source": [
    "# use resnet15 to extract features\n",
    "# use kmeans to cluster players\n",
    "\n",
    "resnet = models.mobilenet_v3_large(pretrained=True)\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = torch.stack([transform(img) for img in all_players])\n",
    "dataset = dataset.to(device)\n",
    "\n",
    "features = resnet(dataset).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   6,  11,  16,  20, 301,  41, 192, 305,  27])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find cosine similarity between each pair of players to the first player\n",
    "player_1 = features[0]\n",
    "cosine_similarities = F.cosine_similarity(torch.tensor(player_1), torch.tensor(features))\n",
    "\n",
    "# find the top 5 most similar players, and their indices\n",
    "top_5_indices = cosine_similarities.argsort(descending=True)[:10]\n",
    "\n",
    "top_5_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision_cuda111",
   "language": "python",
   "name": "vision_cuda111"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
