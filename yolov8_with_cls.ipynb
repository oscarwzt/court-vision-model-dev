{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import random\n",
    "from IPython.display import HTML\n",
    "import torchvision.models as torch_models\n",
    "from base64 import b64encode\n",
    "import os\n",
    "from IPython.display import Video\n",
    "from utils import *\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from cls_detection import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else device\n",
    "print(device)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "yolo_model_path = \"weights/detect_large.pt\"\n",
    "model = YOLO(yolo_model_path)\n",
    "classNames = ['basketball', 'hoop', 'person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    }
   ],
   "source": [
    "all_pos_vid = \"video_test_dataset/all_made.mp4\"\n",
    "all_neg_vid = \"video_test_dataset/all_miss.mp4\"\n",
    "\n",
    "all_models = [\n",
    "    \"cls_chkpoint_resnet18/checkpoint_2023-12-24-15-10_lr_0.0001_batch_64/best_model.pth\",\n",
    "    \"cls_chkpoint_resnet18/checkpoint_2023-12-24-15-02_lr_0.001_batch_64/best_model.pth\",\n",
    "    \"cls_chkpoint_resnet18/checkpoint_2023-12-24-15-27_lr_0.0005_batch_64/best_model.pth\",\n",
    "    ]\n",
    "\n",
    "batch_size = 128 \n",
    "conf = 0.4\n",
    "device = \"cuda:1\"\n",
    "\n",
    "for model_path in all_models:\n",
    "    pos_results = inference_by_batch(yolo_model_path,\n",
    "                    model_path,\n",
    "                    video_path = all_pos_vid,\n",
    "                    save_result_vid = False,\n",
    "                    display_result = False,\n",
    "                    batch_size = batch_size,\n",
    "                    show_progress=True,\n",
    "                    cls_conf_threshold=conf,\n",
    "                    device=device,\n",
    "                    model_type=\"resnet18\"\n",
    "                    )\n",
    "    pos_score = len(pos_results)\n",
    "    \n",
    "    neg_results = inference_by_batch(yolo_model_path,\n",
    "                    model_path,\n",
    "                    video_path = all_neg_vid,\n",
    "                    save_result_vid = False,\n",
    "                    display_result = False,\n",
    "                    batch_size = batch_size,\n",
    "                    show_progress=True,\n",
    "                    cls_conf_threshold=conf,\n",
    "                    device=device,\n",
    "                    model_type=\"resnet18\"\n",
    "                    )\n",
    "    neg_score = len(neg_results)\n",
    "    print(f\"{model_path}: pos_score: {pos_score}, neg_score: {neg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Models loaded!\n",
      "Initializing video capture...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [01:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m \n\u001b[1;32m      8\u001b[0m conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43minference_by_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43myolo_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mall_models\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideo_test_dataset/0/RPReplay_Final1702697672 4.MOV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msave_result_vid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdisplay_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcls_conf_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/zw2688/DL_project/cls_detection.py:145\u001b[0m, in \u001b[0;36minference_by_batch\u001b[0;34m(yolo_model_path, cls_model_path, video_path, cls_conf_threshold, detect_conf_threshold, save_result_vid, output_dir, saved_video_name, batch_size, display_result, show_progress, skip_to_sec, show_score_prob, cls_img_size, device, model_type)\u001b[0m\n\u001b[1;32m    143\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(img, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProb: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(display_prob)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m140\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m2\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_result_vid:\n\u001b[0;32m--> 145\u001b[0m         \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_models = [\n",
    "    \"cls_chkpoint_resnet18/checkpoint_2023-12-24-15-10_lr_0.0001_batch_64/best_model.pth\",\n",
    "    \"cls_chkpoint_resnet18/checkpoint_2023-12-24-15-02_lr_0.001_batch_64/best_model.pth\",\n",
    "    \"cls_chkpoint_resnet18/checkpoint_2023-12-24-15-27_lr_0.0005_batch_64/best_model.pth\",\n",
    "    ]\n",
    "\n",
    "batch_size = 128 \n",
    "conf = 0.4\n",
    "\n",
    "inference_by_batch(yolo_model_path,\n",
    "                    all_models[0],\n",
    "                    video_path = \"video_test_dataset/0/RPReplay_Final1702697672 4.MOV\",\n",
    "                    save_result_vid = True,\n",
    "                    display_result = True,\n",
    "                    batch_size = 128 * 2,\n",
    "                    show_progress=True,\n",
    "                    cls_conf_threshold=conf,\n",
    "                    device=\"cuda:1\",\n",
    "                    model_type=\"resnet18\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_predict_image(cls_model, img, preprocess, device, threshold = 0.5):\n",
    "    input_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        cls_output = cls_model(input_tensor)\n",
    "    probability = torch.sigmoid(cls_output.squeeze())\n",
    "\n",
    "    # prob, predicted_class = torch.max(probability, dim=0)\n",
    "    # return predicted_class.item(), prob.item()\n",
    "    return probability[1] > threshold, probability\n",
    "\n",
    "def predict_hoop_box(img, cls_model, x1, y1, x2, y2, preprocess, device, threshold = 0.5):\n",
    "    cropped_img = img[y1:y2, x1:x2]\n",
    "    cropped_img_pil = Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_RGB2BGR))\n",
    "    # Preprocess the cropped image\n",
    "    predicted_class, prob = cls_predict_batch(cls_model, cropped_img_pil, preprocess, device, threshold)\n",
    "    return cropped_img_pil, predicted_class, prob\n",
    "\n",
    "def cls_predict_batch(cls_model, batch_imgs, preprocess, device, threshold = 0.5):\n",
    "    # Process and batch images\n",
    "    # check if batch_imgs is a list.\n",
    "        \n",
    "    if isinstance(batch_imgs, list):\n",
    "        batch_tensor = torch.stack([preprocess(img) for img in batch_imgs])\n",
    "        batch_tensor = batch_tensor.to(device)\n",
    "    else:\n",
    "        batch_tensor = preprocess(batch_imgs).unsqueeze(0).to(device)\n",
    "\n",
    "    # Forward pass for the whole batch\n",
    "    with torch.no_grad():\n",
    "        cls_output = cls_model(batch_tensor)\n",
    "\n",
    "    # Calculate probabilities and predicted classes\n",
    "    probabilities = torch.sigmoid(cls_output)\n",
    "    return probabilities > threshold, probabilities[:, 1].tolist()\n",
    "\n",
    "def predict_hoop_box_batch(img, cls_model,  preprocess, device, threshold = 0.5):\n",
    "    cropped_imgs_pil = []\n",
    "\n",
    "    for cropped_img in img:\n",
    "        cropped_img_pil = Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_RGB2BGR))\n",
    "        cropped_imgs_pil.append(cropped_img_pil)\n",
    "\n",
    "    return cls_predict_batch(cls_model, cropped_imgs_pil, preprocess, device, threshold)\n",
    "\n",
    "\n",
    "def inference_by_batch(yolo_model_path,\n",
    "                       cls_model_path,\n",
    "                       video_path, \n",
    "                       cls_conf_threshold = 0.6,\n",
    "                       detect_conf_threshold = 0.4,\n",
    "                       save_result_vid = False, \n",
    "                       output_dir = None, \n",
    "                       saved_video_name = None,\n",
    "                       batch_size=128,\n",
    "                       display_result = False,\n",
    "                       show_progress = True,\n",
    "                       skip_to_sec = 0,\n",
    "                       show_score_prob = False,\n",
    "                       cls_img_size = 112,\n",
    "                       device = device,\n",
    "                       model_type = \"resnet50\"\n",
    "                       ):\n",
    "    preprocess = transforms.Compose([\n",
    "                transforms.Resize((cls_img_size, cls_img_size)),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "    \n",
    "    print(\"Loading models...\")\n",
    "    model = YOLO(yolo_model_path)\n",
    "    cls_model = load_resnet50(cls_model_path, device=device) if model_type == \"resnet50\" else load_resnet18(cls_model_path, device=device)\n",
    "    cls_model.to(device)    \n",
    "    cls_model.eval()\n",
    "    print(\"Models loaded!\")\n",
    "    \n",
    "    cap, fps, frame_width, frame_height = get_video_info(video_path)\n",
    "    if skip_to_sec > 0:\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, skip_to_sec * 1000)\n",
    "        \n",
    "    num_skiped_frames = int(skip_to_sec * fps)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - num_skiped_frames\n",
    "    \n",
    "    print(\"Initializing video capture...\")\n",
    "    if save_result_vid:\n",
    "        video_name = video_path.split(\"/\")[-1]\n",
    "        video_name = video_name.split(\".\")[0] + \".mp4\"\n",
    "\n",
    "        if saved_video_name is not None:\n",
    "            output_path = saved_video_name if output_dir is None else os.path.join(output_dir, saved_video_name)\n",
    "        else:\n",
    "            output_path = \"inferenced_\" + video_name if output_dir is None else os.path.join(output_dir, \"inferenced_\" + video_name)\n",
    "            \n",
    "        codec = cv2.VideoWriter_fourcc(*'vp09')\n",
    "        out = cv2.VideoWriter(output_path, codec, fps, (frame_width,frame_height))\n",
    "    \n",
    "    num_batches = math.ceil(total_frames / batch_size)\n",
    "\n",
    "    results = []\n",
    "    score_timestamps = []\n",
    "    \n",
    "    count = 0\n",
    "    score = 0\n",
    "    display_prob = [0.0]\n",
    "    \n",
    "    if show_progress:\n",
    "        batch_range = tqdm(range(num_batches))\n",
    "    else:\n",
    "        batch_range = range(num_batches)\n",
    "\n",
    "    for i in batch_range:\n",
    "        frames = []\n",
    "        for i in range(batch_size):\n",
    "            ret, img = cap.read()\n",
    "            if ret:\n",
    "                frames.append(img)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if frames:\n",
    "            results = model(frames, \n",
    "                            stream=False, \n",
    "                            verbose = False, \n",
    "                            conf=detect_conf_threshold,\n",
    "                            device=device)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        print(\"Finished detecting objects in batch\", i + 1, \"out of\", num_batches)\n",
    "\n",
    "        for c, r in tqdm(enumerate(results)):\n",
    "            print(c)\n",
    "            img = r.orig_img\n",
    "            boxes = r.boxes\n",
    "            cropped_images = []\n",
    "            count += 1\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # convert to int values\n",
    "                confidence = box.conf.item()\n",
    "                predicted_class = model.names[int(box.cls)] \n",
    "                if predicted_class == \"hoop\":\n",
    "                    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                    cv2.putText(img, f'{predicted_class}: {confidence:.3f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                    \n",
    "                    if x1 > x2 or y1 > y2:\n",
    "                        continue\n",
    "                    else:\n",
    "                        cropped_img = img[y1:y2, x1:x2]\n",
    "                        cropped_images.append(cropped_img)\n",
    "                        \n",
    "                if predicted_class == \"basketball\":\n",
    "                    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    cv2.putText(img, f'{predicted_class}: {confidence:.3f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    \n",
    "            \n",
    "            if len(cropped_images) == 0:\n",
    "                continue\n",
    "            pred, prob = predict_hoop_box_batch(cropped_images, cls_model,  preprocess, device, threshold=cls_conf_threshold)\n",
    "            if pred.sum() > 0 and count > 60:\n",
    "                score += 1\n",
    "                count = 0\n",
    "                current_frame = i * batch_size + c\n",
    "                time_stamp = current_frame / fps\n",
    "                score_timestamps.append((time_stamp, prob))\n",
    "                display_prob = prob\n",
    "        \n",
    "            cv2.putText(img, f'Score: {score}', (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2)\n",
    "            if show_score_prob:\n",
    "                cv2.putText(img, f'Prob: {max(display_prob):.3f}', (10, 140), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2)\n",
    "            if save_result_vid:\n",
    "                out.write(img)\n",
    "        print(\"finished inferencing with cls\")\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "    if save_result_vid:\n",
    "        print(\"releasing video writer\")\n",
    "        out.release()\n",
    "    print(\"releasing video capture\")\n",
    "    cap.release()\n",
    "    \n",
    "\n",
    "    if display_result:\n",
    "        display(Video(output_path, embed=True))\n",
    "        return score_timestamps, output_path\n",
    "    else:\n",
    "        return score_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 97)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(\"classification_dataset_groupby_env/mima/0/2023_06_21_-_Game_2-xJLCPqvNo00_mp4-1_jpg.rf.9a5945913f60f3140f47637565fc1c4e.jpg\").size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
