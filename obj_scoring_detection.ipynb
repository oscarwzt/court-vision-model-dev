{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from video_utils import *\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "from obj_detection_utils import *\n",
    "from get_yt_vids import *\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"weights/detect_large.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"full_videos/NM9_fvYsWME.mp4\"\n",
    "conf = 0.4\n",
    "skip_to_sec = 0\n",
    "end_at_sec = 1e10\n",
    "batch_size = 64\n",
    "show_progress = True\n",
    "\n",
    "cap, fps, frame_width, frame_height, total_frames = initialize_video_capture(video_path=video_path, skip_to_sec = skip_to_sec)\n",
    "out, output_path = initialize_video_writer(fps = fps,\n",
    "                                           video_dimension= (frame_width, frame_height),\n",
    "                                           video_path=video_path,\n",
    "                                           saved_video_name=\"output.mp4\"\n",
    "                                           )\n",
    "\n",
    "num_batches = math.ceil(total_frames / batch_size)\n",
    "reached_stopping_time = False\n",
    "box_containing_ball_prev = None\n",
    "score = 0\n",
    "no_relevant_ball = True\n",
    "last_scored_time = -1\n",
    "\n",
    "# Initialize the progress bar if needed\n",
    "if show_progress:\n",
    "    batch_range = tqdm(range(num_batches), desc='Processing Batches')\n",
    "else:\n",
    "    batch_range = range(num_batches)\n",
    "\n",
    "timestamps = []\n",
    "\n",
    "for i in batch_range:\n",
    "    start_time = time.time()  # Start time for fps calculation\n",
    "    if reached_stopping_time:\n",
    "        break\n",
    "    frames = []\n",
    "    for _ in range(batch_size):\n",
    "        ret, img = cap.read()\n",
    "        if ret:\n",
    "            frames.append(img)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if frames:\n",
    "        results = model(frames, \n",
    "                        stream=False, \n",
    "                        verbose = False, \n",
    "                        conf=conf,\n",
    "                        device=device)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    for idx, (frame, r) in enumerate(zip(frames, results)):\n",
    "\n",
    "        current_frame_num = idx + i * batch_size\n",
    "        current_time = skip_to_sec + current_frame_num / fps\n",
    "\n",
    "        \n",
    "        if current_time >= end_at_sec:\n",
    "            reached_stopping_time = True\n",
    "            break\n",
    "        \n",
    "        cv2.putText(frame, f\"Score: {score}\", (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 255, 255), 2)\n",
    "        boxes = r.boxes\n",
    "        \n",
    "        \n",
    "        bounding_boxes = boxes.xyxy.cpu().numpy().astype(int)\n",
    "\n",
    "        labels = [model.names[i] for i in boxes.cls.cpu().numpy()]\n",
    "        objects = {label: [] for label in labels}\n",
    "        \n",
    "\n",
    "        for box, label in zip(bounding_boxes, labels):\n",
    "            objects[label].append(box)\n",
    "        \n",
    "        if \"basketball\" not in objects or \"hoop\" not in objects:\n",
    "            out.write(frame)       \n",
    "            continue\n",
    "        hoop_boxes = objects[\"hoop\"]\n",
    "        detection_areas = [get_detection_box(*box) for box in hoop_boxes]\n",
    "        entry_boxes = [get_entry_box(*box) for box in hoop_boxes]\n",
    "        exit_boxes = [get_exit_box(*box) for box in hoop_boxes]\n",
    "        ball_centers = [get_center(*box) for box in objects[\"basketball\"]]\n",
    "        # relevant_ball_centers = [center for center in ball_centers \n",
    "        #                                 for det_area in detection_areas\n",
    "        #                                 if is_in_box(*center, *det_area)]\n",
    "        relevant_ball_boxes = [box for box in objects[\"basketball\"] \n",
    "                                        for det_area in detection_areas\n",
    "                                        if is_in_box(*box, *det_area)]\n",
    "        if not relevant_ball_boxes:\n",
    "            no_relevant_ball = True\n",
    "            out.write(frame)\n",
    "            continue\n",
    "        else:\n",
    "            no_relevant_ball = False\n",
    "\n",
    "        for ball_boxes in objects[\"basketball\"]:\n",
    "            cv2.circle(frame, get_center(*ball_boxes), 5, COLORS[\"basketball\"], -1)\n",
    "        focus_areas = {\n",
    "            \"detection_area\": detection_areas,\n",
    "            \"hoop_box\": hoop_boxes,\n",
    "            \"entry_box\": entry_boxes,\n",
    "            \"exit_box\": exit_boxes\n",
    "        }\n",
    "        box_containing_ball_cur = None\n",
    "        # determine which box the ball is in\n",
    "        for box_name, all_boxes in focus_areas.items():\n",
    "            for box in all_boxes:\n",
    "                if any([is_in_box(*relevant_ball_boxes, *box, threshold=0.5) for relevant_ball_boxes in relevant_ball_boxes]):\n",
    "                    box_containing_ball_cur = box_name #if not no_relevant_ball else None\n",
    "                    cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), COLORS[box_name], 2)\n",
    "                    cv2.putText(frame, box_name, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, COLORS[box_name], 2)\n",
    "                else:\n",
    "                    cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 0, 0), 2)\n",
    "                    cv2.putText(frame, box_name, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "        ball_in_interested_area = (box_containing_ball_cur == \"hoop_box\" or box_containing_ball_cur == \"exit_box\")\n",
    "        \n",
    "        time_since_last_scored = current_time - last_scored_time\n",
    "        if box_containing_ball_prev == \"entry_box\" and ball_in_interested_area and time_since_last_scored > 1:\n",
    "            score += 1\n",
    "            last_scored_time = current_time\n",
    "            timestamps.append(current_time)\n",
    "            \n",
    "        box_containing_ball_prev = box_containing_ball_cur\n",
    "\n",
    "                    \n",
    "  \n",
    "        cv2.putText(frame, f\"Score: {score}\", (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 255, 255), 2)\n",
    "        out.write(frame)  \n",
    "    if show_progress:\n",
    "        elapsed_time = time.time() - start_time  # Elapsed time for batch\n",
    "        fps = batch_size / elapsed_time  # Calculate fps based on batches processed\n",
    "        batch_range.set_postfix(fps=f\"{fps:.2f} fps\", refresh=True)          \n",
    "            \n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "display_video(output_path, width = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scoring_timestamps(video_path = None,\n",
    "                           url = None, \n",
    "                           model = model,\n",
    "                           skip_to_sec = 0,\n",
    "                           batch_size = 16,\n",
    "                           show_progress = True,\n",
    "                            ):\n",
    "    if not video_path and not url:\n",
    "        raise ValueError(\"Either video_path or url must be provided\")\n",
    "    elif url and not video_path:\n",
    "        video_path = download_video(url, \"testing_videos\")\n",
    "    cap, fps, frame_width, frame_height, total_frames = initialize_video_capture(video_path=video_path, skip_to_sec = skip_to_sec)\n",
    "\n",
    "    num_batches = math.ceil(total_frames / batch_size)\n",
    "\n",
    "    box_containing_ball_prev = None\n",
    "    score = 0\n",
    "\n",
    "    time_since_last_score = np.inf\n",
    "    frame_since_last_score = 2 ** 10\n",
    "    if show_progress:\n",
    "        batch_range = tqdm(range(num_batches))\n",
    "    else:\n",
    "        batch_range = range(num_batches)\n",
    "\n",
    "    timestamps = []\n",
    "    for i in batch_range:\n",
    "        frames = []\n",
    "        for i in range(batch_size):\n",
    "            ret, img = cap.read()\n",
    "            if ret:\n",
    "                frames.append(img)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if frames:\n",
    "            results = model(frames, \n",
    "                            stream=False, \n",
    "                            verbose = False, \n",
    "                            conf=conf,\n",
    "                            device=device)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "            \n",
    "            bounding_boxes = boxes.xyxy.cpu().numpy()\n",
    "            bounding_boxes = bounding_boxes.astype(int)\n",
    "            labels = [model.names[i] for i in boxes.cls.cpu().numpy()]\n",
    "            \n",
    "            objects = {label: [] for label in labels}\n",
    "            \n",
    "\n",
    "            for box, label in zip(bounding_boxes, labels):\n",
    "                objects[label].append(box)\n",
    "            \n",
    "            if \"basketball\" not in objects or \"hoop\" not in objects:     \n",
    "                continue\n",
    "            hoop_box = objects[\"hoop\"]\n",
    "            detection_area = [get_detection_box(*box) for box in hoop_box]\n",
    "            entry_box = [get_entry_box(*box) for box in hoop_box]\n",
    "            exit_box = [get_exit_box(*box) for box in hoop_box]\n",
    "            ball_center = [get_center(*box) for box in objects[\"basketball\"]]\n",
    "            relevant_ball_centers = [center for center in ball_center \n",
    "                                            for det_area in detection_area\n",
    "                                            if is_in_box(*center, *det_area)]\n",
    "            if not relevant_ball_centers:\n",
    "                continue\n",
    "\n",
    "            focus_areas = {\n",
    "                \"detection_area\": detection_area,\n",
    "                \"hoop_box\": hoop_box,\n",
    "                \"entry_box\": entry_box,\n",
    "                \"exit_box\": exit_box\n",
    "            }\n",
    "            box_containing_ball_cur = None\n",
    "            # determine which box the ball is in\n",
    "            for box_name, all_boxes in focus_areas.items():\n",
    "                for box in all_boxes:\n",
    "                    if any([is_in_box(*relevant_ball_center, *box) for relevant_ball_center in relevant_ball_centers]):\n",
    "                        box_containing_ball_cur = box_name \n",
    "\n",
    "            ball_in_interested_area = (box_containing_ball_cur == \"hoop_box\" or box_containing_ball_cur == \"exit_box\")\n",
    "            time_since_last_score = frame_since_last_score / fps\n",
    "            if box_containing_ball_prev == \"entry_box\" and ball_in_interested_area and time_since_last_score > 2:\n",
    "                score += 1\n",
    "                frame_since_last_score = 0\n",
    "                timestamps.append(cap.get(cv2.CAP_PROP_POS_MSEC) / 1000)\n",
    "                \n",
    "                \n",
    "            box_containing_ball_prev = box_containing_ball_cur\n",
    "            frame_since_last_score += 1         \n",
    "                \n",
    "    cap.release()\n",
    "    \n",
    "    timestamps = [t + skip_to_sec for t in timestamps]\n",
    "    \n",
    "    return video_path, timestamps\n",
    "\n",
    "def trim_highlights_from_timestamps(video_path,\n",
    "                      score_timestamps, \n",
    "                      clip_start_offset = 6, # number of seconds before scoring\n",
    "                      clip_end_offset = 2,   # number of seconds after scoring\n",
    "                      output_path = \".\",\n",
    "                      ffmpeg_path = \"ffmpeg-git-20240203-amd64-static/ffmpeg\"):\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    video_name = os.path.basename(video_path)\n",
    "    video_name = os.path.splitext(video_name)[0]\n",
    "    \n",
    "    for i, timestamp in enumerate(score_timestamps):\n",
    "        start_time = max(0, timestamp - clip_start_offset)\n",
    "        end_time = timestamp + clip_end_offset\n",
    "        clip_output_path = os.path.join(output_path, f\"{video_name}_highlight_{i}.mp4\")\n",
    "\n",
    "        # Construct FFmpeg command for trimming\n",
    "        ffmpeg_command = [ffmpeg_path, '-i', video_path, '-ss', str(start_time), '-to', str(end_time), '-c', 'copy', clip_output_path]\n",
    "        subprocess.run(ffmpeg_command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "def generate_highlights(video_path = None,\n",
    "                           url = None, \n",
    "                           model = model,\n",
    "                           skip_to_sec = 0,\n",
    "                           batch_size = 16,\n",
    "                           show_progress = True,\n",
    "                           highlight_output_path = \"highlights\"\n",
    "                           ):\n",
    "                        \n",
    "    video_path, timestamps = get_scoring_timestamps(video_path = video_path,\n",
    "                           url = url, \n",
    "                           model = model,\n",
    "                           skip_to_sec = skip_to_sec,\n",
    "                           batch_size = batch_size,\n",
    "                           show_progress = show_progress,\n",
    "                            )\n",
    "    print(f\"found {len(timestamps)} highlights\")\n",
    "    \n",
    "    print(\"starting to trim highlights\")\n",
    "    trim_highlights_from_timestamps(video_path, timestamps, output_path = highlight_output_path)\n",
    "    print(\"finished trimming highlights\")\n",
    "\n",
    "                           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(url, save_path, resolution=None, ffmpeg_path = \"ffmpeg-git-20240203-amd64-static/ffmpeg\"):\n",
    "    # Define download options for yt-dlp\n",
    "    ydl_opts = {\n",
    "        'outtmpl': os.path.join(save_path, '%(id)s.%(ext)s'),\n",
    "        'format': 'bestvideo',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegVideoConvertor',\n",
    "            'preferedformat': 'mp4',  # Convert to mp4 if necessary\n",
    "            \n",
    "        }],  \n",
    "        \"ffmpeg_location\": f\"{ffmpeg_path}\"\n",
    "    }\n",
    "    \n",
    "    # If a specific resolution is requested, adjust the format selection\n",
    "    if resolution:\n",
    "        ydl_opts['format'] = f'bestvideo[height<={resolution}]'\n",
    "    else:\n",
    "        # Ensure the format is set to mp4 for consistency and compatibility\n",
    "        ydl_opts['format'] += '[ext=mp4]'\n",
    "\n",
    "    # Ensure the save directory exists\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    video_id = get_yt_video_id(url)\n",
    "    video_path = os.path.join(save_path, f\"{video_id}.mp4\")\n",
    "    if os.path.isfile(video_path):\n",
    "        print(f'Video {video_id} already exists.')\n",
    "        return video_path\n",
    "    # Download the video\n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        video_info = ydl.extract_info(url, download=True)\n",
    "        video_id = video_info.get('id')\n",
    "        \n",
    "    return video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(url, save_path, resolution=None):\n",
    "    yt = YouTube(url)\n",
    "    if resolution:\n",
    "        video = yt.streams.filter(mime_type=\"video/mp4\", res = resolution).first()\n",
    "    else:\n",
    "        video = yt.streams.filter(mime_type=\"video/mp4\").order_by(\"resolution\").desc().first()\n",
    "\n",
    "    video_id = get_yt_video_id(url)\n",
    "    video_path = os.path.join(save_path, f\"{video_id}.mp4\")\n",
    "    \n",
    "    # if video does not exist, download it\n",
    "    if not os.path.isfile(video_path):\n",
    "        print(f'Downloading video {video_id}...')\n",
    "        video.download(output_path=save_path, filename=video_id+\".mp4\")\n",
    "    else:\n",
    "        print(f'Video {video_id} already exists.')\n",
    "    return video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading video NM9_fvYsWME...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'testing_videos/NM9_fvYsWME.mp4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_video( \"https://www.youtube.com/watch?v=NM9_fvYsWME&t=2s\", \"testing_videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_highlights_from_timestamps(\"testing_videos/NM9_fvYsWME.mp4\", [10, 20, 30], output_path = \"highlights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video NM9_fvYsWME already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 31/407 [00:07<01:32,  4.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m yt_links \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.youtube.com/watch?v=NM9_fvYsWME&t=2s\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.youtube.com/watch?v=4PLIiY_sJTo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.youtube.com/watch?v=5qDqxZhOtlM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.youtube.com/watch?v=w2wkz62PJeY\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m yt_links:\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mgenerate_highlights\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mskip_to_sec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m240\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 135\u001b[0m, in \u001b[0;36mgenerate_highlights\u001b[0;34m(video_path, url, model, skip_to_sec, batch_size, show_progress, highlight_output_path)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_highlights\u001b[39m(video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    127\u001b[0m                            url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m    128\u001b[0m                            model \u001b[38;5;241m=\u001b[39m model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m                            highlight_output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhighlights\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m                            ):\n\u001b[0;32m--> 135\u001b[0m     video_path, timestamps \u001b[38;5;241m=\u001b[39m \u001b[43mget_scoring_timestamps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m                           \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mskip_to_sec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mskip_to_sec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(timestamps)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m highlights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarting to trim highlights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 37\u001b[0m, in \u001b[0;36mget_scoring_timestamps\u001b[0;34m(video_path, url, model, skip_to_sec, batch_size, show_progress)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frames:\n\u001b[0;32m---> 37\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/bigdata/environments/vision_cuda111/lib/python3.10/site-packages/ultralytics/engine/model.py:155\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    An alias for the predict method, enabling the model instance to be callable.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m        (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bigdata/environments/vision_cuda111/lib/python3.10/site-packages/ultralytics/engine/model.py:406\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bigdata/environments/vision_cuda111/lib/python3.10/site-packages/ultralytics/engine/predictor.py:204\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bigdata/environments/vision_cuda111/lib/python3.10/site-packages/torch/autograd/grad_mode.py:43\u001b[0m, in \u001b[0;36m_DecoratorContextManager._wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 43\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/bigdata/environments/vision_cuda111/lib/python3.10/site-packages/ultralytics/engine/predictor.py:282\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(im0s)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    283\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n",
      "File \u001b[0;32m~/bigdata/environments/vision_cuda111/lib/python3.10/site-packages/ultralytics/utils/ops.py:52\u001b[0m, in \u001b[0;36mProfile.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Stop timing.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart  \u001b[38;5;66;03m# delta-time\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt\n",
      "File \u001b[0;32m~/bigdata/environments/vision_cuda111/lib/python3.10/site-packages/ultralytics/utils/ops.py:62\u001b[0m, in \u001b[0;36mProfile.time\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get current time.\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda:\n\u001b[0;32m---> 62\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/bigdata/environments/vision_cuda111/lib/python3.10/site-packages/torch/cuda/__init__.py:496\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    494\u001b[0m _lazy_init()\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "yt_links = [\n",
    "    \"https://www.youtube.com/watch?v=NM9_fvYsWME&t=2s\",\n",
    "    \"https://www.youtube.com/watch?v=4PLIiY_sJTo\",\n",
    "    \"https://www.youtube.com/watch?v=5qDqxZhOtlM\",\n",
    "    \"https://www.youtube.com/watch?v=w2wkz62PJeY\",\n",
    "]\n",
    "\n",
    "for link in yt_links:\n",
    "    generate_highlights(url = link,\n",
    "                           batch_size= 16,\n",
    "                           skip_to_sec = 240)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
